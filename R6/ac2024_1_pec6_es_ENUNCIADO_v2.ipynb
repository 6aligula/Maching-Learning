{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k17EFI32k2_1"
      },
      "source": [
        "<div style=\"width: 100%; clear: both;\">\n",
        "<div style=\"float: left; width: 50%;\">\n",
        "</div>\n",
        "<div style=\"float: right; width: 50%;\">\n",
        "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">22.418 · Aprendizaje automático</p>\n",
        "<p style=\"margin: 0; text-align:right;\">Grado en Ciencia de Datos Aplicada</p>\n",
        "<p style=\"margin: 0; texto-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
        "</div>\n",
        "</div>\n",
        "<div style=\"width:100%;\">&nbsp;</div>\n",
        "\n",
        "\n",
        "# PEC 6: APRENDIZAJE POR REFUERZO\n",
        "\n",
        "## Introducción\n",
        "\n",
        "En esta actividad estudiaremos cómo utilizar las técnicas previamente estudiadas en teoría para la resolución de problemas prácticos.\n",
        "\n",
        "Las competencias asociadas a este módulo son las siguientes:\n",
        "\n",
        "- Diseñar un marco experimental teniendo en cuenta los métodos más adecuados para la captura, procesamiento, almacenamiento, análisis y visualización de datos.\n",
        "- Utilizar de forma combinada los fundamentos matemáticos, estadísticos y de programación para desarrollar soluciones a problemas en el ámbito de la ciencia de los datos.\n",
        "\n",
        "Y los objetivos que perseguimos con este *notebook* de prácticas son:\n",
        "\n",
        "- Conocer los principales métodos de aprendizaje automático en problemas de aprendizaje por refuerzo, y saber aplicar el tipo de algoritmo adecuado en cada situación, así como sus puntos fuertes y debilidades.\n",
        "- Identificar los distintos elementos que aparecen en un proyecto de aprendizaje por refuerzo.\n",
        "- Familiarización con librerías de programación de redes neuronales a través de ejemplos.\n",
        "- Saber desarrollar una solución para un problema haciendo uso de métodos de aprendizaje por refuerzo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCBOaTN9-414"
      },
      "source": [
        "## Descripción de la PAC\n",
        "\n",
        "En esta PAC trabajaremos con el problema llamado MountainCar, un entorno clásico de control dentro del ecosistema de Gymnasium. El objetivo principal es que un coche suba una pendiente suficientemente alta, a pesar de tener un motor débil que no le permite, por sí solo, ascender directamente hasta la cima. Esto hace que la estrategia óptima suele consistir en oscilar entre ambos lados para tomar suficiente impulso.\n",
        "\n",
        "El entorno MountainCar-v0* es la versión **discreta**, donde el coche puede elegir entre tres acciones: empujar hacia la izquierda, no empujar o empujar hacia la derecha. En cambio, en la versión **continua** (*MountainCarContinuous-v0*), el agente puede aplicar cualquier fuerza dentro de un intervalo real \\([-1, 1]\\), lo que complica el diseño de soluciones: ya no hay un número finito de acciones fácil de enumerar y, por tanto, se necesitan aproximaciones o discretizaciones más sofisticadas. A medida que exploramos el entorno, veremos cómo estas dos variantes requieren enfoques diferentes, especialmente en lo que se refiere al uso de tablas Q o de algoritmos capaces de manejar espacios de acción continuos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku2NJOkhm0tJ"
      },
      "source": [
        "En esta actividad, al igual que en los ejemplos incluidos en los materiales de la asignatura, utilizaremos la API para trabajar con entornos y agentes de aprendizaje reforzado ***gym***, mantenida actualmente por el proyecto ***Gymnasium** *.\n",
        "\n",
        "https://gymnasium.farama.org/\n",
        "\n",
        "Ejecuta la siguiente celda para instalar e importar la librería."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4vtFpUqm0PL"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium\n",
        "\n",
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leqV7AZJmFPL"
      },
      "source": [
        "## Ejercicio 1: Reflexión inicial e implementación naïf [5 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4iFPMMcrUr_"
      },
      "source": [
        "### **Ejercicio 1.1 [1 punto] Reflexión inicial**\n",
        "Reflexiona sobre las principales diferencias entre MountainCar-v0 (acciones discretas) y MountainCarContinuous-v0 (acciones continuas). ¿Por qué es más complicado controlar la acción en caso continuo? ¿Cómo afecta a la creación de una Q table?\n",
        "\n",
        "Revisa la documentación en los siguientes links:\n",
        "- https://gymnasium.farama.org/environments/classic_control/mountain_car/\n",
        "- https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC1JVCTdrY1D"
      },
      "source": [
        "% pon tu respuesta aquí\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqlf3Th7raGj"
      },
      "source": [
        "### **Ejercicio 1.2 [1 punto] Implementación Naïf Discreta**\n",
        "\n",
        "Implementa una política naïf para el entorno MountainCar-v0 (acciones **discretas**). La política debería:\n",
        "\n",
        "* Empujar hacia la derecha si la velocidad es positiva.\n",
        "* Empujar hacia la izquierda si la velocidad es negativa.\n",
        "* Definir un criterio sencillo para el caso de velocidad cero (por ejemplo empujar a la derecha si la posición es inferior a 0, oa la izquierda si es superior).\n",
        "\n",
        "\n",
        "Una vez implementada la política:\n",
        "* Guarda la recompensa y el número de pasos seguidos en cada ejecución\n",
        "* Guarda el vídeo de cada ejecución\n",
        "* Ejecuta el entorno 10 veces\n",
        "* Visualiza un vídeo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99J8gFjVkv-c"
      },
      "outputs": [],
      "source": [
        "# pon tu respuesta aquí"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtHw5n4_n2ny"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elsEBD9KyuXb"
      },
      "source": [
        "### **Ejercicio 1.3 [1 punto] Reflexión Naïf Discreta**\n",
        "\n",
        "Explica los resultados obtenidos con esta política:\n",
        "\n",
        " - ¿Qué tipo de recompensas se han obtenido en los episodios de ejecución?\n",
        " - ¿Ha conseguido el coche llegar a lo alto de la montaña? ¿Cuál es el instante en el que se acaba el episodio normalmente?\n",
        " - Observa el vídeo generado (si lo has grabado) y describe cómo es el movimiento del coche.\n",
        "\n",
        "Justifica porque esta política raramente resuelve el problema de forma óptima.\n",
        "\n",
        " - ¿Qué le falta a la política para que el coche consiga suficiente inercia hacia la derecha?\n",
        " - ¿Por qué crees que el episodio se termina por timeout (en su caso)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk_38yuxmqMT"
      },
      "source": [
        "% pon tu respuesta aquí\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjkTOKq7s5_y"
      },
      "source": [
        "### **Ejercicio 1.4 [1 punto] Implementación Naïf continua**\n",
        "\n",
        "Implementa una política naïf para el entorno MountainCarContinuous-v0 (acciones **continues**). La política debería:\n",
        "\n",
        "* Empujar hacia la derecha si la velocidad es positiva.\n",
        "* Empujar hacia la izquierda si la velocidad es negativa.\n",
        "* Definir un criterio sencillo para el caso de velocidad cero (por ejemplo empujar a la derecha si la posición es inferior a 0, oa la izquierda si es superior).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Muestra el código y explica brevemente la lógica seguida. Guarda el resultado de una ejecución en un vídeo y visualízalo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSlOPLFM07wZ"
      },
      "outputs": [],
      "source": [
        "# pon tu respuesta aquí"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJTnM3DGpKwO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN8G7cdQ1hDa"
      },
      "source": [
        "### **Ejercicio 1.5 [1 punto] Comparación de resultados**\n",
        "\n",
        "\n",
        "Compara los resultados obtenidos al ejecutar la política naïf (basada en el signo de la velocidad) en dos escenarios distintos de MountainCar: la versión discreta y la versión continua. Observa:\n",
        "\n",
        " - Recompensas por episodio\n",
        " - Número de pasos por episodio\n",
        " - Recompensa media y pasos medios\n",
        "\n",
        "Responde a las siguientes preguntas:\n",
        "\n",
        " - ¿Qué diferencias existen entre el comportamiento de la política naïf en la versión discreta y en la versión continua?\n",
        " - ¿Por qué crees que MountainCarContinuous-v0 parece conseguir recompensas positivas y, en cambio, en la versión discreta obtiene recompensas fuertemente negativas?\n",
        " - Interpreta cómo influye el espacio de acción (discreto vs. continuo) en el movimiento del coche y en su capacidad para alcanzar el objetivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwB4wDuI1gsI"
      },
      "source": [
        "% pon tu respuesta aquí"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dv2XCCNtR60"
      },
      "source": [
        "## Ejercicio 2: implementación con Aprendizaje-Q para entrenar al agente [5 puntos]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtMVI9StDPNI"
      },
      "source": [
        "### Ejercicio 2.1: implementación con Aprendizaje-Q para entrenar al agente [2 puntos]\n",
        "\n",
        "En este ejercicio, debes crear y entrenar una Q-Table para resolver MountainCar-v0 (discreto). La solución debe incluir:\n",
        "\n",
        " * Discretización del estado: Define un número de “buckets” para la posición y la velocidad. Explica cómo realizas la conversión de valores reales a índices enteros.\n",
        " * Q-Table: Inicia con valores cero. Indica cuáles serán sus dimensiones y estructura.\n",
        " * Entrenamiento con Q-learning: Implementa la fórmula de Q-learning, explicando cada parámetro que ajustas y por qué. Proporciona evidencias de convergencia mediante recompensas acumuladas o pasos.\n",
        " * Política epsilon-greedy: Empiezas con una \\(\\epsilon\\) alta y redúcela progresivamente. Detalla el valor inicial, la tasa de decrecimiento y el mínimo que se permite.\n",
        " * Graba un vídeo: Haz una prueba con \\(\\epsilon = 0\\) al final del entrenamiento y muestra el comportamiento del agente. Indica claramente dónde se puede acceder al vídeo y cómo se generan los archivos.\n",
        "\n",
        "> **Pista**: Puedes empezar con 20 buckets para la posición y 20 para la velocidad, unos 10000 episodios, \\(\\alpha = 0.1\\) y \\(\\gamma = 0.99\\). Ajusta si es necesario para mejorar resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNu21KOYtuoy"
      },
      "source": [
        "\n",
        "**Pseudocódigo de alto nivel**\n",
        "\n",
        "```python\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "# 1. CREACIÓN DEL ENTORNO\n",
        "env = gym.make(\"MountainCar-v0\")  \n",
        "# - Prepara el entorno con acciones discretas (3 opciones)\n",
        "\n",
        "# 2. DISCRETIZACIÓN DEL ESTADO\n",
        "# - Decide cuántos \"buckets\" usas para (posición, velocidad)\n",
        "num_buckets_pos = 20\n",
        "num_buckets_vel = 20\n",
        "\n",
        "def discretiza_estado(observacion):\n",
        "    # - Convierte la posición y velocidad en índices enteros dentro de los buckets\n",
        "    # - Retorna (idx_pos, idx_vel)\n",
        "    ...\n",
        "\n",
        "# 3. INICIALIZACIÓN DE LA Q-TABLE\n",
        "n_acciones = env.action_space.n   # 3 acciones\n",
        "Q = np.zeros((num_buckets_pos, num_buckets_vel, n_acciones))\n",
        "# - Tendrá un valor Q para cada combinación (pos_bucket, vel_bucket, acción)\n",
        "\n",
        "# 4. PARÁMETROS DE APRENDIZAJE\n",
        "alpha = 0.1      # Tasa de aprendizaje\n",
        "gamma = 0.99     # Factor de descuento\n",
        "epsilon = 1.0    # Porcentaje exploración\n",
        "eps_decay = 0.995\n",
        "eps_min = 0.01\n",
        "episodios = 10000\n",
        "\n",
        "# 5. BUCLE PRINCIPAL DE ENTRENAMIENTO\n",
        "for e in range(episodios):\n",
        "    # a) REINICIO DEL ENTORNO\n",
        "    observacion, _ = env.reset()\n",
        "    estado = discretiza_estado(observacion)\n",
        "    terminado = False\n",
        "\n",
        "    # b) MIENTRAS EL EPISODIO NO HAYA TERMINADO\n",
        "    while not terminado:\n",
        "        # - Seleccionar la acción con epsilon-greedy (explorar o explotar)\n",
        "        ...\n",
        "        # - Ejecutar env.step(accion) y observar la nueva observación y recompensa\n",
        "        ...\n",
        "        # - Discretizar el siguiente estado\n",
        "        ...\n",
        "        # - Actualizar la Q-table usando la fórmula de Q-learning\n",
        "        ...\n",
        "        # - Actualizar el valor de 'estado' al nuevo estado discretizado\n",
        "        ...\n",
        "\n",
        "    # c) DECREMENTO DE EPSILON\n",
        "    if epsilon > eps_min:\n",
        "        epsilon *= eps_decay\n",
        "\n",
        "# 6. CIERRA EL ENTORNO\n",
        "env.close()\n",
        "\n",
        "# 7. EJECUCIÓN FINAL (CON EPSILON=0) + VÍDEO\n",
        "# - Repetir algunos episodios sin explorar y grabar el comportamiento del agente\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5KKQiNy22jk"
      },
      "outputs": [],
      "source": [
        "# pon tu código aquí"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QrY2AttvA0T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZlzeNVyENOJ"
      },
      "source": [
        "### Ejercicio 2.2: Visualización de la q-table [1 punto]\n",
        "\n",
        "Crea una visualización que permita observar el comportamiento de la Q-Table entrenada. Detalle:\n",
        "\n",
        " * Crea un mapa de calor que muestre el valor de cada acción en función de los buckets de posición y velocidad.\n",
        " * Representa visualmente las acciones seleccionadas como óptimas por la política. Explica cómo se traduce la acción (p. ej., 0 = empuje izquierdo).\n",
        " * Proporciona una interpretación de los patrones observados y relacionalos con los resultados del 2.1 (zonas dominantes para cada acción)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzvGygGWphrk"
      },
      "outputs": [],
      "source": [
        "# pon tu código aquí"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iI42Lojm3k-6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0kBb0GxE922"
      },
      "source": [
        "### Ejercicio 2.3: Análisis de la q-table [1 punto]\n",
        "\n",
        "Una vez visualizada la Q-Table:\n",
        "\n",
        " * Analiza los patrones de movimiento según los buckets de posición y velocidad.\n",
        " * Compara los resultados con los del Ejercicio 1 (política manual). Responde:\n",
        " * ¿Cómo se distribuyen las acciones óptimas?\n",
        " * ¿El tiempo total para completar un episodio mejora con la política derivada de Q-learning?\n",
        " * ¿Hay estados donde \"no hacer nada\" (acción 1) se considera óptimo? Cuenta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16W3YlECKu1F"
      },
      "source": [
        "% pon tu código aquí"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RICAAmjR6xSb"
      },
      "source": [
        "### Ejercicio 2.4: Búsqueda del número óptimo de parámetros [1 punto]\n",
        "\n",
        "Ahora queremos determinar qué discretización de los estados da mejores resultados. Por eso:\n",
        "\n",
        " * Ejecuta el código del Ejercicio 2.1 con diferentes números de buckets: [5, 10, 50, 100].\n",
        " * Mide el número medio de pasos necesarios para completar un episodio.\n",
        " * Representa gráficamente cómo varía el número de pasos en función del número de buckets.\n",
        " Responde:\n",
        " * ¿Qué impacto tiene un mayor número de buckets?\n",
        " * ¿Es siempre mejor aumentar el número de buckets? Justifica.\n",
        " * Compara los resultados obtenidos con los del ejercicio 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI8QjKmX6yLM"
      },
      "outputs": [],
      "source": [
        "# pon tu código aquí"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMCW2sIq6xP6"
      },
      "source": [
        "% analiza los resultados obtenidos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbehuz2aKJbl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "env_ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
